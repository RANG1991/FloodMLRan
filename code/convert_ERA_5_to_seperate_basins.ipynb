{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_PVOFFFzJXh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "# from google.colab import drive\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import datetime\n",
    "#!pip install geopandas\n",
    "import geopandas as gpd\n",
    "# !pip install climata\n",
    "from climata.usgs import DailyValueIO\n",
    "from climata.usgs import InstantValueIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6aelJmUzfO7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TEST_PERIOD = (\"1989-10-01\", \"1999-09-30\")\n",
    "TRAINING_PERIOD = (\"1999-10-01\", \"2008-09-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pjaZ83ozfY8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_if_discharge_file_exists(basin_id, ERA5_discharge_data_folder_name):\n",
    "    return Path(ERA5_discharge_data_folder_name +'/dis_' + station_id + '.csv').exists()\n",
    "\n",
    "\n",
    "def check_if_all_percip_files_exist(basin_id, output_folder_name):\n",
    "    shape_file = output_folder_name + '/shape_' + station_id + '.csv'\n",
    "    time24_file = output_folder_name + '/time24_' + station_id + '.npy'\n",
    "    percip24_file = output_folder_name + '/precip24_' + station_id\n",
    "    timedis24_file = output_folder_name + '/timedis24_' + station_id + '.npy'\n",
    "    dis24_file = output_folder_name + '/dis24_' + station_id\n",
    "    info_file = output_folder_name + '/info_' + station_id + '.txt'\n",
    "    latlon_file = output_folder_name + '/latlon_' + station_id + '.csv'\n",
    "    list_files = [shape_file, time24_file, percip24_file, timedis24_file, dis24_file, info_file, latlon_file]\n",
    "    all_files_exist = all(lambda file_path: Paht(file_path).exists() for file_name in list_files)\n",
    "    return all_files_exist\n",
    "\n",
    "\n",
    "def parse_single_basin_discharge(station_id, output_folder_name):\n",
    "    dis_file_exists = check_if_discharge_file_exists(station_id, output_folder_name)\n",
    "    if dis_file_exists:\n",
    "      print(\"The discharge file of the basin: {} exists\".format(station_id))\n",
    "      return\n",
    "    start_time = pd.to_datetime('1980-01-01 00:00:00')\n",
    "    end_time = pd.to_datetime('2021-09-15 00:00:00')\n",
    "    param_id = \"00060\"\n",
    "    data = InstantValueIO(\n",
    "        start_date=start_time,\n",
    "        end_date=end_time,\n",
    "        station=station_id,\n",
    "        parameter=param_id)\n",
    "\n",
    "    FT2M = 0.3048\n",
    "    for series in data:\n",
    "        flow = [r[1] for r in series.data]\n",
    "        datetimes = [r[0] for r in series.data]\n",
    "\n",
    "    ls = []\n",
    "    for i in range(0, len(datetimes)):\n",
    "      t = datetimes[i].astimezone(datetime.timezone.utc)\n",
    "      ls.append([t.year,t.month,t.day,t.hour,t.minute,flow[i]*(FT2M**3)])\n",
    "\n",
    "    df = pd.DataFrame(data=ls, columns=['year','month','day','hour','minute','flow'])\n",
    "    df_group = df.groupby(by=['year','month','day','hour']).mean()\n",
    "    df_group = df_group.assign(minute=0)\n",
    "    df_group.loc[df_group['flow'] < 0 , 'flow'] = np.nan\n",
    "\n",
    "    filename = output_folder_name +'/dis_' + station_id + '.csv'\n",
    "    df_group.to_csv(filename)\n",
    "\n",
    "    \n",
    "def parse_single_basin_percipitation(station_id, basin_data, discharge_file_name, ERA5_data_folder_name, output_folder_name):\n",
    "    all_files_exist = check_if_all_percip_files_exist(station_id, output_folder_name)\n",
    "    if all_files_exist:\n",
    "      print(\"all percipitation file of the basin: {} exists\".format(station_id))\n",
    "      return\n",
    "    basin_boundaries = basin_data.bounds\n",
    "    # get the minimum and maximum longitude and latitude (square boundaries)\n",
    "    min_lon = np.squeeze(np.floor((basin_boundaries['minx'].values * 10) / 10))\n",
    "    min_lat = np.squeeze(np.floor((basin_boundaries['miny'].values * 10) / 10))\n",
    "    max_lon = np.squeeze(np.ceil((basin_boundaries['maxx'].values * 10) / 10))\n",
    "    max_lat = np.squeeze(np.ceil((basin_boundaries['maxy'].values * 10) / 10))\n",
    "\n",
    "    # read the discharge of the required station\n",
    "    df_dis = pd.read_csv(discharge_file_name)\n",
    "    # convert the columns of year, month, day, hour, minute to datetime and put it as the dataframe index\n",
    "    df_dis.index = [datetime.datetime(df_dis['year'][i], df_dis['month'][i],\n",
    "                                      df_dis['day'][i], df_dis['hour'][i],\n",
    "                                      df_dis['minute'][i]) for i in range(0, len(df_dis))]\n",
    "    print(df_dis)\n",
    "    # read the precipitation of the required station\n",
    "    # ERA5\n",
    "    year_start = df_dis['year'].min() - 1\n",
    "    year_end = df_dis['year'].max()\n",
    "    print(year_start, year_end)\n",
    "    list_of_dates = []\n",
    "    list_of_total_precipitations = []\n",
    "    for year in range(year_start, year_end + 1):\n",
    "        print(year)\n",
    "        fn = f\"{ERA5_data_folder_name}/tp_US_{year}.nc\"\n",
    "        dataset = nc.Dataset(fn)\n",
    "        # ti is an array containing the dates as the number of hours since 1900-01-01 00:00\n",
    "        # e.g. - [780168, 780169, 780170, ...]\n",
    "        ti = dataset['time'][:]\n",
    "        if year == year_start:\n",
    "            lon = dataset['longitude'][:]\n",
    "            lat = dataset['latitude'][:]\n",
    "            max_lon_array = lon.max()\n",
    "            min_lon_array = lon.min()\n",
    "            max_lat_array = lat.max()\n",
    "            min_lat_array = lat.min()\n",
    "            ind_lon_min = np.squeeze(np.argwhere(lon == max(min_lon, min_lon_array)))\n",
    "            ind_lon_max = np.squeeze(np.argwhere(lon == min(max_lon, max_lon_array)))\n",
    "            ind_lat_min = np.squeeze(np.argwhere(lat == max(min_lat, min_lat_array)))\n",
    "            ind_lat_max = np.squeeze(np.argwhere(lat == min(max_lat, max_lat_array)))\n",
    "        # multiply by 1000 to convert from meter to mm\n",
    "        tp = np.asarray(dataset['tp'][:, ind_lat_max:ind_lat_min + 1, ind_lon_min:ind_lon_max + 1]) * 1000\n",
    "        # convert the time to datetime format and append it to the times array\n",
    "        times = [datetime.datetime.strptime(\"1900-01-01 00:00\", \"%Y-%m-%d %H:%M\") + datetime.timedelta(hours=int(ti[i]))\n",
    "                 for i in range(0, len(ti))]\n",
    "        times = np.asarray(times)\n",
    "        list_of_dates.append(times)\n",
    "        list_of_total_precipitations.append(tp)\n",
    "\n",
    "    # concatenate the datetimes from all the years\n",
    "    datetimes = np.concatenate(list_of_dates, axis=0)\n",
    "\n",
    "    # concatenate the percipitation data from all the years\n",
    "    precip = np.concatenate(list_of_total_precipitations, axis=0)\n",
    "\n",
    "    print([station_id, lat[ind_lat_min], lat[ind_lat_max], lon[ind_lon_min],\n",
    "           lon[ind_lon_max], precip.shape])\n",
    "\n",
    "    df = pd.DataFrame(data=datetimes, index=datetimes)\n",
    "    datetimes = df.index.to_pydatetime()\n",
    "\n",
    "    ls = [[precip[i, :, :]] for i in range(0, len(datetimes))]\n",
    "    df = pd.DataFrame(data=ls, index=datetimes, columns=['precip'])\n",
    "\n",
    "    # down sample the datetime data into 1D (1 day) bins and sum the values falling into the same bin\n",
    "    df1 = df.resample('1D').sum()\n",
    "    datetimes24 = df1.index.to_pydatetime()\n",
    "    precip24 = np.stack(df1['precip'].values)\n",
    "\n",
    "    dis = df_dis['flow'].values\n",
    "    datetimesdis = df_dis.index.to_pydatetime()\n",
    "\n",
    "    # downsample the datetime data into 1D (1 day) bins and take the mean\n",
    "    # of the values falling into the same bin\n",
    "    df1 = df_dis.resample('1D').mean()\n",
    "    datetimesdis24 = df1.index.to_pydatetime()\n",
    "    dis24 = np.stack(df1['flow'].values)\n",
    "\n",
    "    fn = output_folder_name + '/shape_' + station_id + '.csv'\n",
    "    pd.DataFrame(data=np.array([(precip24.shape[0],), (precip24.shape[1],), (precip24.shape[2],)]).T,\n",
    "                 columns=['time', 'lat', 'lon'], index=[1]).to_csv(fn)\n",
    "    fn = output_folder_name + '/time24_' + station_id + '.npy'\n",
    "    np.save(fn, datetimes24)\n",
    "    fn = output_folder_name + '/precip24_' + station_id\n",
    "    precip24.tofile(fn)\n",
    "    fn = output_folder_name + '/timedis24_' + station_id + '.npy'\n",
    "    np.save(fn, datetimesdis24)\n",
    "    fn = output_folder_name + '/dis24_' + station_id\n",
    "    dis24.tofile(fn)\n",
    "    fn = output_folder_name + '/info_' + station_id + '.txt'\n",
    "    with open(fn, 'w') as f:\n",
    "        print(precip.shape[0], precip.shape[1], precip.shape[2], lat[ind_lat_min], lat[ind_lat_max], lon[ind_lon_min],\n",
    "              lon[ind_lon_max], file=f)\n",
    "\n",
    "    lonb = lon[ind_lon_min:ind_lon_max]\n",
    "    latb = lat[ind_lat_max:ind_lat_min]\n",
    "    lslon = [lonb[i] for i in range(0, len(lonb)) for j in range(0, len(latb))]\n",
    "    lslat = [latb[j] for i in range(0, len(lonb)) for j in range(0, len(latb))]\n",
    "    lat_lon_lst = []\n",
    "    for i in range(0, len(lslon)):\n",
    "        if np.squeeze(basin_data['geometry'].contains(Point(lslon[i], lslat[i]))):\n",
    "            lat_lon_lst.append([lslat[i], lslon[i]])\n",
    "\n",
    "    fn = output_folder_name + '/latlon_' + station_id + '.csv'\n",
    "    pd.DataFrame(data=lat_lon_lst, columns=['lat', 'lon']).to_csv(fn, index=False, float_format='%6.1f')\n",
    "\n",
    "    fn = output_folder_name + '/info_' + station_id + '.txt'\n",
    "    with open(fn, 'w') as f:\n",
    "        print(precip.shape[0], precip.shape[1], precip.shape[2], lat[ind_lat_min], lat[ind_lat_max], lon[ind_lon_min],\n",
    "              lon[ind_lon_max], file=f)\n",
    "\n",
    "\n",
    "def main():\n",
    "    root_folder = \"/sci/labs/efratmorin/lab_share/FloodsML/data/ERA5/\"\n",
    "    boundaries_file_name = root_folder + \"/HCDN_nhru_final_671.shp\"\n",
    "    ERA5_percip_data_folder_name = root_folder + \"/Percipitation/\"\n",
    "    ERA5_discharge_data_folder_name = root_folder + \"/Discharge/\"\n",
    "    output_folder_name = root_folder + \"/ERA_5_all_data/\"\n",
    "    # read the basins' boundaries file using gpd.read_file()\n",
    "    basins_data = gpd.read_file(boundaries_file_name)\n",
    "    for station_id in basins_data[\"hru_id\"]:\n",
    "      station_id = str(station_id).zfill(8)\n",
    "      print(\"working on basin with id: {}\".format(station_id))\n",
    "      parse_single_basin_discharge(station_id, ERA5_discharge_data_folder_name)\n",
    "      discharge_file_name = ERA5_discharge_data_folder_name + '/dis_' + str(station_id) + '.csv'\n",
    "      # get the boundaries of the required basin using its station ID\n",
    "      basin_data = basins_data[basins_data['hru_id'] == int(station_id)]\n",
    "      parse_single_basin_percipitation(station_id, basin_data, discharge_file_name, ERA5_percip_data_folder_name, output_folder_name)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "WtQvmhszrGqT"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convert_ERA_5_to_seperate_basins.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FloodsML",
   "language": "python",
   "name": "floodsml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}