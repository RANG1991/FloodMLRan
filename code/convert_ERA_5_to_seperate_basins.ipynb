{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"convert_ERA_5_to_seperate_basins.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"I_PVOFFFzJXh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"64d38292-d0d3-4724-8a67-e8b15bb4c265","executionInfo":{"status":"ok","timestamp":1652095339754,"user_tz":-180,"elapsed":13623,"user":{"displayName":"Ran Galun","userId":"04055554830780091933"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting geopandas\n","  Downloading geopandas-0.10.2-py2.py3-none-any.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 5.0 MB/s \n","\u001b[?25hCollecting fiona>=1.8\n","  Downloading Fiona-1.8.21-cp37-cp37m-manylinux2014_x86_64.whl (16.7 MB)\n","\u001b[K     |████████████████████████████████| 16.7 MB 43.7 MB/s \n","\u001b[?25hRequirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.1.post1)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.3.5)\n","Collecting pyproj>=2.2.0\n","  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n","\u001b[K     |████████████████████████████████| 6.3 MB 40.8 MB/s \n","\u001b[?25hCollecting munch\n","  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n","Collecting cligj>=0.5\n","  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2021.10.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n","Collecting click-plugins>=1.0\n","  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.4.0)\n","Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n","Installing collected packages: munch, cligj, click-plugins, pyproj, fiona, geopandas\n","Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.21 geopandas-0.10.2 munch-2.5.0 pyproj-3.2.1\n","Mounted at /content/drive\n"]}],"source":["import xarray as xr\n","from os.path import isfile, join\n","from os import listdir\n","from google.colab import drive\n","import torch\n","import pandas as pd\n","import numpy as np\n","import netCDF4 as nc\n","import datetime\n","!pip install geopandas\n","import geopandas as gpd\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["TEST_PERIOD = (\"1989-10-01\", \"1999-09-30\")\n","TRAINING_PERIOD = (\"1999-10-01\", \"2008-09-30\")"],"metadata":{"id":"y6aelJmUzfO7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_single_basin(station_id, basin_boundaries, discharge_file_name, ERA5_data_folder_name, output_folder):\n","\n","    # get the minimum and maximum longitude and latitude (square boundaries)\n","    min_lon = np.squeeze(np.floor((basin_boundaries['minx'].values * 10) / 10))\n","    min_lat = np.squeeze(np.floor((basin_boundaries['miny'].values * 10) / 10))\n","    max_lon = np.squeeze(np.ceil((basin_boundaries['maxx'].values * 10) / 10))\n","    max_lat = np.squeeze(np.ceil((basin_boundaries['maxy'].values * 10) / 10))\n","\n","    # read the discharge of the required station\n","    df_dis = pd.read_csv(discharge_file_name)\n","    # convert the columns of year, month, day, hour, minute to datetime and put it as the dataframe index\n","    df_dis.index = [datetime.datetime(df_dis['year'][i], df_dis['month'][i],\n","                                      df_dis['day'][i], df_dis['hour'][i],\n","                                      df_dis['minute'][i]) for i in range(0, len(df_dis))]\n","    print(df_dis)\n","\n","    # read the precipitation of the required station\n","    # ERA5\n","    year_start = df_dis['year'].min() - 1\n","    year_end = df_dis['year'].max()\n","    print(year_start, year_end)\n","    list_of_dates = []\n","    list_of_total_precipitations = []\n","    for year in range(year_start, year_end + 1):\n","        print(year)\n","        fn = f\"{ERA5_data_folder_name}/tp_US_{year}.nc\"\n","        dataset = nc.Dataset(fn)\n","        # ti is an array containing the dates as the number of hours since 1900-01-01 00:00\n","        # e.g. - [780168, 780169, 780170, ...]\n","        ti = dataset['time'][:]\n","        if year == year_start:\n","            lon = dataset['longitude'][:]\n","            lat = dataset['latitude'][:]\n","            max_lon_array = lon.max()\n","            min_lon_array = lon.min()\n","            max_lat_array = lat.max()\n","            min_lat_array = lat.min()\n","            ind_lon_min = np.squeeze(np.argwhere(lon == max(min_lon, min_lon_array)))\n","            ind_lon_max = np.squeeze(np.argwhere(lon == min(max_lon, max_lon_array)))\n","            ind_lat_min = np.squeeze(np.argwhere(lat == max(min_lat, min_lat_array)))\n","            ind_lat_max = np.squeeze(np.argwhere(lat == min(max_lat, max_lat_array)))\n","        # multiply by 1000 to convert from meter to mm\n","        tp = np.asarray(dataset['tp'][:, ind_lat_max:ind_lat_min + 1, ind_lon_min:ind_lon_max + 1]) * 1000\n","        # convert the time to datetime format and append it to the times array\n","        times = [datetime.datetime.strptime(\"1900-01-01 00:00\", \"%Y-%m-%d %H:%M\") + datetime.timedelta(hours=int(ti[i]))\n","                 for i in range(0, len(ti))]\n","        times = np.asarray(times)\n","        list_of_dates.append(times)\n","        list_of_total_precipitations.append(tp)\n","\n","    # concatenate the datetimes from all the years\n","    datetimes = np.concatenate(list_of_dates, axis=0)\n","\n","    # concatenate the percipitation data from all the years\n","    precip = np.concatenate(list_of_total_precipitations, axis=0)\n","\n","    print([station_id, lat[ind_lat_min], lat[ind_lat_max], lon[ind_lon_min],\n","           lon[ind_lon_max], precip.shape])\n","\n","    df = pd.DataFrame(data=datetimes, index=datetimes)\n","    datetimes = df.index.to_pydatetime()\n","\n","    ls = [[precip[i, :, :]] for i in range(0, len(datetimes))]\n","    df = pd.DataFrame(data=ls, index=datetimes, columns=['precip'])\n","\n","    # down sample the datetime data into 1D (1 day) bins and sum the values falling into the same bin\n","    df1 = df.resample('1D').sum()\n","    datetimes24 = df1.index.to_pydatetime()\n","    precip24 = np.stack(df1['precip'].values)\n","\n","    dis = df_dis['flow'].values\n","    datetimesdis = df_dis.index.to_pydatetime()\n","\n","    # downsample the datetime data into 1D (1 day) bins and take the mean\n","    # of the values falling into the same bin\n","    df1 = df_dis.resample('1D').mean()\n","    datetimesdis24 = df1.index.to_pydatetime()\n","    dis24 = np.stack(df1['flow'].values)\n","\n","    fn = output_folder + '/ERA_5_all_data/shape_' + station_id + '.csv'\n","    pd.DataFrame(data=[precip24.shape[0], precip24.shape[1], precip24.shape[2]],\n","                 columns=['time', 'lat', 'lon']).to_csv(fn, index=False)\n","    fn = output_folder + '/ERA_5_all_data/time24_' + station_id + '.npy'\n","    np.save(fn, datetimes24)\n","    fn = output_folder + '/ERA_5_all_data/precip24_' + station_id\n","    precip24.tofile(fn)\n","    fn = output_folder + '/ERA_5_all_data/timedis24_' + station_id + '.npy'\n","    np.save(fn, datetimesdis24)\n","    fn = output_folder + '/ERA_5_all_data/dis24_' + station_id\n","    dis24.tofile(fn)\n","    fn = output_folder + '/ERA_5_all_data/info_' + station_id + '.txt'\n","    with open(fn, 'w') as f:\n","        print(precip.shape[0], precip.shape[1], precip.shape[2], lat[ind_lat_min], lat[ind_lat_max], lon[ind_lon_min],\n","              lon[ind_lon_max], file=f)\n","\n","    from shapely.geometry import Point\n","\n","    lonb = lon[ind_lon_min:ind_lon_max]\n","    latb = lat[ind_lat_max:ind_lat_min]\n","    lslon = [lonb[i] for i in range(0, len(lonb)) for j in range(0, len(latb))]\n","    lslat = [latb[j] for i in range(0, len(lonb)) for j in range(0, len(latb))]\n","    lat_lon_lst = []\n","    for i in range(0, len(lslon)):\n","        if np.squeeze(basin['geometry'].contains(Point(lslon[i], lslat[i]))):\n","            lat_lon_lst.append([lslat[i], lslon[i]])\n","\n","    fn = output_folder + '/ERA_5_all_data/latlon_' + station_id + '.csv'\n","    pd.DataFrame(data=lat_lon_lst, columns=['lat', 'lon']).to_csv(fn, index=False, float_format='%6.1f')\n","\n","    fn = output_folder + '/ERA_5_all_data/info_' + station_id + '.txt'\n","    with open(fn, 'w') as f:\n","        print(precip.shape[0], precip.shape[1], precip.shape[2], lat[ind_lat_min], lat[ind_lat_max], lon[ind_lon_min],\n","              lon[ind_lon_max], file=f)\n","\n","\n","def preprocess_train_test_data(data):\n","    data_df = data.to_pandas()\n","    if len(data[\"time\"].data) > 0:\n","        data_df[\"tp\"] = data_df[\"tp\"].apply(lambda x: x * 1000)\n","        data_df = data_df.reset_index()\n","        data_df[\"time\"] = pd.to_datetime(data_df[\"time\"], infer_datetime_format=True)\n","        data_df = data_df.resample('1D', on='time').sum()\n","        data_df = data_df.reset_index()\n","    return data_df\n","\n","\n","def preprocess_single_year_file(basin_filename):\n","    year_dataset = xr.load_dataset(basin_filename)\n","    print(year_dataset)\n","    year_dataset = year_dataset.sum([\"longitude\", \"latitude\"])\n","    test_data = year_dataset.sel(time=slice(*TEST_PERIOD))\n","    train_data = year_dataset.sel(time=slice(*TRAINING_PERIOD))\n","    preprocessed_test_data = preprocess_train_test_data(test_data)\n","    preprocessed_train_data = preprocess_train_test_data(train_data)\n","    return preprocessed_train_data, preprocessed_test_data\n","\n","\n","def preprocess_data(basins_files_dir):\n","    basins_files = [f for f in listdir(basins_files_dir) if isfile(join(basins_files_dir, f))]\n","    df_train = pd.DataFrame(columns=[\"time\", \"tp\"])\n","    df_test = pd.DataFrame(columns=[\"time\", \"tp\"])\n","    for i in range(len(basins_files)):\n","        print(f'processing file {i + 1} from {len(basins_files)} files')\n","        basin_filename = join(basins_files_dir, basins_files[i])\n","        preprocessed_train_data, preprocessed_test_data = preprocess_single_year_file(basin_filename)\n","        df_train = pd.concat([df_train, preprocessed_train_data])\n","        df_test = pd.concat([df_test, preprocessed_test_data])\n","    df_train = df_train.sort_values(by=[\"time\"], ascending=True)\n","    df_train.to_csv(\"./df_train.csv\")\n","    df_test = df_test.sort_values(by=[\"time\"], ascending=True)\n","    df_test.to_csv(\"./df_test.csv\")\n","\n","\n","def main():\n","    boundaries_file_name =  \"drive/Computers/My_Laptop_X1/HCDN_nhru_final_671.shp\"\n","    ERA5_data_folder_name = \"drive/My Drive/ERA5/\"\n","    output_folder_name = \"drive/My Drive/ERA5/\"\n","    # read the basins' boundaries file using gpd.read_file()\n","    basin_data = gpd.read_file(boundaries_file_name)\n","    for station_id in basin_data[\"hru_id\"]:\n","      discharge_file_name = '/Streamflow/dis_' + station_id + '.csv'\n","      # get the boundaries of the required basin using its station ID\n","      basin = basin_data[basin_data['hru_id'] == int(station_id)]\n","      bounds = basin.bounds\n","      read_single_basin(station_id, bounds, discharge_file_name, ERA5_data_folder_name, output_folder_name)\n","\n","main()"],"metadata":{"id":"1pjaZ83ozfY8"},"execution_count":null,"outputs":[]}]}